{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aiNDDQp_G9DE"
      },
      "source": [
        "##Собственный"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Uxg2ADmG8ml"
      },
      "outputs": [],
      "source": [
        "!unzip /content/drive/MyDrive/slovo.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r08Vkk2AG_fe"
      },
      "source": [
        "##Kaggle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SJNl0qsZt0xd"
      },
      "outputs": [],
      "source": [
        "! pip install -q kaggle\n",
        "from google.colab import files\n",
        "files.upload()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bUagDZT0t5r7"
      },
      "outputs": [],
      "source": [
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I5gsmacLt8ig"
      },
      "outputs": [],
      "source": [
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zoPRX3A3ywp0"
      },
      "outputs": [],
      "source": [
        "!kaggle kernels pull kleinsbotle/usage-example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y3f1Zt3duVe3",
        "outputId": "77a8d377-f22b-4f5d-b9f6-4888f9707b33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading slovo.zip to /content\n",
            "100% 14.8G/14.8G [02:52<00:00, 153MB/s]\n",
            "100% 14.8G/14.8G [02:52<00:00, 91.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d kapitanov/slovo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PBVfjP2pujLm"
      },
      "outputs": [],
      "source": [
        "! unzip /content/slovo.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SdhzWTqy8Fp2"
      },
      "source": [
        "##Обучаем"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pOke-bbPQQjU"
      },
      "source": [
        "Для работы со свежей версией torch (Если надо сохранять в ONNX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dN9SoxZpS0Gu",
        "outputId": "01a24a23-efc5-46db-c23e-0cd5e087815c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.2.1+cu121\n",
            "12.1\n"
          ]
        }
      ],
      "source": [
        "!python -c 'import torch;print(torch.__version__);print(torch.version.cuda)'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "Y6bRmWOPQD2k"
      },
      "outputs": [],
      "source": [
        "!pip install -U openmim\n",
        "!mim install mmengine\n",
        "#!mim install mmcv\n",
        "!pip install mmcv==2.1.0 -f https://download.openmmlab.com/mmcv/dist/cu121/torch2.1/index.html\n",
        "! git clone https://github.com/open-mmlab/mmaction2.git\n",
        "%cd mmaction2\n",
        "! pip install -v -e .\n",
        "!pip install timm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhNzLIg7QX66"
      },
      "source": [
        "Для работы с версией torch==1.12.0 (Если надо обучать)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ERhbLlkSTsL"
      },
      "outputs": [],
      "source": [
        "!pip install torch==1.12.0 torchvision --extra-index-url https://download.pytorch.org/whl/cu113\n",
        "!pip install -U openmim\n",
        "!mim install mmengine\n",
        "!mim install 'mmcv >= 2.0.0, <2.2.0'\n",
        "! git clone https://github.com/open-mmlab/mmaction2.git\n",
        "%cd mmaction2\n",
        "! pip install -v -e .\n",
        "!pip install timm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wmBS1IWGSgt3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import cv2\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from glob import glob\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "DATA_DIR = '/content/slovo'\n",
        "TRAIN_DIR = os.path.join(DATA_DIR, 'train')\n",
        "TEST_DIR = os.path.join(DATA_DIR, 'test')\n",
        "ANNOTATIONS_DIR = os.path.join(DATA_DIR, 'annotations')\n",
        "\n",
        "ann = pd.read_csv(os.path.join(DATA_DIR, 'annotations.csv'), sep='\\t')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TAbvu5LTZ59",
        "outputId": "3ff526a5-87b5-4d51-de43-42854918b1b6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1400/1400 [00:02<00:00, 642.69it/s]\n"
          ]
        }
      ],
      "source": [
        "train_files = sorted(glob(os.path.join(TRAIN_DIR, '*')))\n",
        "test_files = sorted(glob(os.path.join(TEST_DIR, '*')))\n",
        "NUM_CLASSES = len(ann['text'].unique()) # Including \"no-action\" class\n",
        "classes = {label: label_id for label, label_id in zip(ann['text'].unique(), range(NUM_CLASSES))}\n",
        "\n",
        "ann_train = []\n",
        "ann_test = []\n",
        "\n",
        "for file in tqdm(train_files + test_files):\n",
        "    video_id = file.split('/')[-1][:-4]\n",
        "    label = ann[ann['attachment_id'] == video_id]['text'].to_string(index=False)\n",
        "    class_id = classes[label]\n",
        "    line = file + ' ' + str(class_id) + '\\n'\n",
        "    if ann[ann['attachment_id'] == video_id]['train'].bool():\n",
        "        ann_train.append(line)\n",
        "    else:\n",
        "        ann_test.append(line)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TqNsHMAVTbAe"
      },
      "outputs": [],
      "source": [
        "with open('ann_train.txt', 'w') as train_file, open('ann_test.txt', 'w') as test_file:\n",
        "    train_file.writelines(ann_train)\n",
        "    test_file.writelines(ann_test)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile mvit-slovo.py\n",
        "\n",
        "# Model settings\n",
        "model = dict(\n",
        "    type='Recognizer3D',\n",
        "    backbone=dict(\n",
        "        type='MViT',\n",
        "        arch='small',\n",
        "        drop_path_rate=0.1,\n",
        "        init_cfg=dict(\n",
        "            type='Pretrained',\n",
        "            checkpoint=\n",
        "            'https://download.openmmlab.com/mmaction/v1.0/recognition/mvit/converted/mvit-small-p244_16x4x1_kinetics400-rgb_20221021-9ebaaeed.pth',\n",
        "            prefix='backbone.')),\n",
        "    data_preprocessor=dict(\n",
        "        type='ActionDataPreprocessor',\n",
        "        mean=[123.675, 116.28, 103.53],\n",
        "        std=[58.395, 57.12, 57.375],\n",
        "        format_shape='NCTHW'),\n",
        "    cls_head=dict(\n",
        "        type='MViTHead',\n",
        "        in_channels=768,\n",
        "        num_classes=51,\n",
        "        label_smooth_eps=0.1,\n",
        "        average_clips='prob'))\n",
        "\n",
        "# Logging settings\n",
        "default_scope = 'mmaction'\n",
        "default_hooks = dict(\n",
        "    runtime_info=dict(type='RuntimeInfoHook'),\n",
        "    timer=dict(type='IterTimerHook'),\n",
        "    logger=dict(type='LoggerHook', interval=525, ignore_last=False),\n",
        "    param_scheduler=dict(type='ParamSchedulerHook'),\n",
        "    checkpoint=dict(\n",
        "        type='CheckpointHook', interval=1, save_best='auto', max_keep_ckpts=5),\n",
        "    sampler_seed=dict(type='DistSamplerSeedHook'),\n",
        "    sync_buffers=dict(type='SyncBuffersHook'))\n",
        "env_cfg = dict(\n",
        "    cudnn_benchmark=False,\n",
        "    mp_cfg=dict(mp_start_method='fork', opencv_num_threads=0),\n",
        "    dist_cfg=dict(backend='nccl'))\n",
        "log_processor = dict(type='LogProcessor', window_size=20, by_epoch=True)\n",
        "vis_backends = [dict(type='TensorboardVisBackend'), dict(type='LocalVisBackend')]\n",
        "visualizer = dict(\n",
        "    type='ActionVisualizer',\n",
        "    vis_backends=vis_backends,\n",
        "    name='visualizer',\n",
        "    save_dir='/kaggle/working/visualization_dir'\n",
        "    )\n",
        "log_level = 'INFO'\n",
        "load_from = None\n",
        "resume = False\n",
        "\n",
        "# Specify dataset paths\n",
        "dataset_type = 'VideoDataset'\n",
        "data_root = '/content/slovo/train'\n",
        "data_root_val = '/content/slovo/test'\n",
        "ann_file_train = '/content/mmaction2/ann_train.txt'\n",
        "ann_file_val = '/content/mmaction2/ann_test.txt'\n",
        "ann_file_test = '/content/mmaction2/ann_test.txt'\n",
        "\n",
        "train_pipeline = [\n",
        "    dict(type='DecordInit', io_backend='disk'),\n",
        "    dict(\n",
        "        type='SampleFrames',\n",
        "        clip_len=16,\n",
        "        frame_interval=4,\n",
        "        num_clips=1,\n",
        "        out_of_bound_opt='repeat_last'),\n",
        "    dict(type='DecordDecode'),\n",
        "    dict(type='Resize', scale=(224, 224)),\n",
        "    dict(type='Flip', flip_ratio=0.5, direction='horizontal'),\n",
        "    dict(type='FormatShape', input_format='NCTHW'),\n",
        "    dict(type='PackActionInputs')\n",
        "]\n",
        "val_pipeline = [\n",
        "    dict(type='DecordInit', io_backend='disk'),\n",
        "    dict(\n",
        "        type='SampleFrames',\n",
        "        clip_len=16,\n",
        "        frame_interval=4,\n",
        "        num_clips=1,\n",
        "        test_mode=True,\n",
        "        out_of_bound_opt='repeat_last'),\n",
        "    dict(type='DecordDecode'),\n",
        "    dict(type='Resize', scale=(224, 224)),\n",
        "    dict(type='FormatShape', input_format='NCTHW'),\n",
        "    dict(type='PackActionInputs')\n",
        "]\n",
        "test_pipeline = [\n",
        "    dict(type='DecordInit', io_backend='disk'),\n",
        "    dict(\n",
        "        type='SampleFrames',\n",
        "        clip_len=16,\n",
        "        frame_interval=4,\n",
        "        num_clips=2,\n",
        "        test_mode=True,\n",
        "        out_of_bound_opt='repeat_last'),\n",
        "    dict(type='DecordDecode'),\n",
        "    dict(type='Resize', scale=(224, 224)),\n",
        "    dict(type='FormatShape', input_format='NCTHW'),\n",
        "    dict(type='PackActionInputs')\n",
        "]\n",
        "\n",
        "train_dataloader = dict(\n",
        "    batch_size=2,\n",
        "    num_workers=2,\n",
        "    persistent_workers=True,\n",
        "    sampler=dict(type='DefaultSampler', shuffle=True),\n",
        "    dataset=dict(\n",
        "        type='VideoDataset',\n",
        "        ann_file=ann_file_train,\n",
        "        data_prefix=dict(video=data_root),\n",
        "        pipeline=train_pipeline))\n",
        "val_dataloader = dict(\n",
        "    batch_size=2,\n",
        "    num_workers=2,\n",
        "    persistent_workers=True,\n",
        "    sampler=dict(type='DefaultSampler', shuffle=False),\n",
        "    dataset=dict(\n",
        "        type='VideoDataset',\n",
        "        ann_file=ann_file_val,\n",
        "        data_prefix=dict(video=data_root_val),\n",
        "        pipeline=val_pipeline,\n",
        "        test_mode=True))\n",
        "test_dataloader = dict(\n",
        "    batch_size=1,\n",
        "    num_workers=2,\n",
        "    persistent_workers=True,\n",
        "    sampler=dict(type='DefaultSampler', shuffle=False),\n",
        "    dataset=dict(\n",
        "        type='VideoDataset',\n",
        "        ann_file=ann_file_test,\n",
        "        data_prefix=dict(video=data_root_val),\n",
        "        pipeline=test_pipeline,\n",
        "        test_mode=True))\n",
        "\n",
        "# Training settigns\n",
        "val_evaluator = dict(type='AccMetric')\n",
        "test_evaluator = dict(type='AccMetric')\n",
        "train_cfg = dict(\n",
        "    type='EpochBasedTrainLoop', max_epochs=25, val_begin=1, val_interval=1)\n",
        "val_cfg = dict(type='ValLoop')\n",
        "test_cfg = dict(type='TestLoop')\n",
        "optim_wrapper = dict(\n",
        "    optimizer=dict(\n",
        "        type='Adam', lr=0.0001, weight_decay=0.0001),\n",
        "    paramwise_cfg=dict(norm_decay_mult=0.0, bias_decay_mult=0.0))\n",
        "param_scheduler = [\n",
        "    dict(\n",
        "        type='MultiStepLR',\n",
        "        by_epoch=True,\n",
        "        begin=0,\n",
        "        end=25,\n",
        "        milestones=[10, 20],\n",
        "        gamma=0.1)\n",
        "]\n",
        "auto_scale_lr = dict(enable=False, base_batch_size=64)\n",
        "dist_params = dict(backend='nccl')\n",
        "launcher = 'pytorch'\n",
        "work_dir = 'work_dirs/mvit-slovo'\n",
        "randomness = dict(seed=None, diff_rank_seed=False, deterministic=False)"
      ],
      "metadata": {
        "id": "Yme2TUxjheg0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "ac9-nS3FtZoM",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "! python tools/train.py ./mvit-slovo.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pU0EB0irtej3"
      },
      "outputs": [],
      "source": [
        "! python tools/test.py mvit-slovo.py work_dirs/mvit-slovo/best_acc_top1_epoch_3.pth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LsIU74_9Wsnb"
      },
      "source": [
        "Сохраняем checkpoint, если нужно"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZgK0bFx19RkR"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from mmaction.apis import inference_recognizer, init_recognizer\n",
        "config_path = '/content/drive/MyDrive/mvit-slovo.py'\n",
        "checkpoint_path = '/content/drive/MyDrive/best_acc_top1_epoch_3.pth'\n",
        "# build the model from a config file and a checkpoint file\n",
        "torch_model = init_recognizer(config_path, checkpoint_path)  # device can be 'cuda:0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "traccjqVKRsm"
      },
      "outputs": [],
      "source": [
        "torch.save(torch_model, '/content/drive/MyDrive/my_model')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/drive/MyDrive/vis_data.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xn55MOaNpYsu",
        "outputId": "69ce3028-5f89-4256-97ea-d8871bd025f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/drive/MyDrive/vis_data.zip\n",
            "  inflating: vis_data/20240522_155651.json  \n",
            "  inflating: vis_data/config.py      \n",
            "  inflating: vis_data/events.out.tfevents.1716393413.4fd840a43d67.190.0  \n",
            "  inflating: vis_data/scalars.json   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sG0Wvq2NW5Gr"
      },
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir /content/vis_data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JkyfUaVK7w0K"
      },
      "source": [
        "#Проверка работоспособности"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3WN31I3irzd"
      },
      "outputs": [],
      "source": [
        "!pip install onnxruntime\n",
        "!pip install onnxscript\n",
        "!pip install onnx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-khbuKoP7vLA"
      },
      "outputs": [],
      "source": [
        "from IPython import display\n",
        "import sys\n",
        "sys.path.append(\"../\")\n",
        "\n",
        "import onnxruntime as ort\n",
        "import torch\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "from drive.MyDrive.Diploma.constants import classes\n",
        "\n",
        "isOnnx = True"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XiTYKcDO7yup"
      },
      "outputs": [],
      "source": [
        "path_to_input_video = \"/content/drive/MyDrive/Diploma/f17a6060-6ced-4bd1-9886-8578cfbb864f.mp4\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ggzm3aYmOtkG"
      },
      "outputs": [],
      "source": [
        "if not isOnnx:\n",
        "  path_to_model = \"/content/drive/MyDrive/Diploma/mvit16-4.pt\"\n",
        "  model = torch.jit.load(path_to_model)\n",
        "  window_size = 16\n",
        "else:\n",
        "  path_to_model = \"/content/drive/MyDrive/Diploma/mvit16-4.onnx\"\n",
        "  session = ort.InferenceSession(path_to_model)\n",
        "  input_name = session.get_inputs()[0].name\n",
        "  input_shape = session.get_inputs()[0].shape\n",
        "  window_size = input_shape[3]\n",
        "  output_names = [output.name for output in session.get_outputs()]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Zu5dBhEKOHY"
      },
      "outputs": [],
      "source": [
        "frame_interval = 4\n",
        "threshold = 0.5\n",
        "mean = [123.675, 116.28, 103.53]\n",
        "std = [58.395, 57.12, 57.375]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lr-NoEZv74fH"
      },
      "outputs": [],
      "source": [
        "def resize(im, new_shape=(224, 224)):\n",
        "    shape = im.shape[:2]  # текущая размерность [height, width]\n",
        "    if isinstance(new_shape, int):\n",
        "        new_shape = (new_shape, new_shape)\n",
        "    # Коэффициент масштабирования (new / old)\n",
        "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
        "    # Вычисляем отступы\n",
        "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
        "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]\n",
        "    dw /= 2\n",
        "    dh /= 2\n",
        "    if shape[::-1] != new_unpad:  # Изменяем размер\n",
        "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
        "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
        "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
        "    # Добавляем границу\n",
        "    value = (114, 114, 114)\n",
        "    bConst = cv2.BORDER_CONSTANT\n",
        "    im = cv2.copyMakeBorder(im, top, bottom, left, right, bConst, value)\n",
        "    im = (im - mean) / std #Стандартизация\n",
        "    return im"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hgKj1pJ76L8",
        "outputId": "59049453-39ed-4a55-c0fa-15fa5fc00053"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Привет!\n"
          ]
        }
      ],
      "source": [
        "cap = cv2.VideoCapture(path_to_input_video)\n",
        "_,frame = cap.read()\n",
        "\n",
        "tensors_list = []\n",
        "prediction_list = []\n",
        "prediction_list.append(\"---\")\n",
        "\n",
        "frame_counter = 0\n",
        "while True:\n",
        "    _, frame = cap.read()\n",
        "    if frame is None:\n",
        "        break\n",
        "    frame_counter += 1\n",
        "    if frame_counter == frame_interval:\n",
        "        image = cv2.cvtColor(frame.copy(), cv2.COLOR_BGR2RGB)\n",
        "        image = resize(image, (224, 224))\n",
        "        image = np.transpose(image, [2, 0, 1])\n",
        "        tensors_list.append(image)\n",
        "        if len(tensors_list) == window_size:\n",
        "            input_tensor = np.stack(tensors_list, axis=1)[None][None]\n",
        "            outputs = session.run(output_names, {input_name: input_tensor.astype(np.float32)})[0]\n",
        "            gloss = str(classes[outputs.argmax()])\n",
        "            if outputs.max() > threshold and gloss != prediction_list[-1] and gloss != \"---\":\n",
        "              prediction_list.append(gloss)\n",
        "              print(gloss)\n",
        "            tensors_list.clear()\n",
        "        frame_counter = 0\n",
        "cap.release()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBsq9mTtB6wg",
        "outputId": "a66666f9-a0b6-480f-ce56-0dce66b12475"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(1, 1, 3, 16, 224, 224)"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "input_tensor.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWxX-ErIPVw7"
      },
      "source": [
        "##Если надо сохранить в ONNX"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rxg80mYuu6FF"
      },
      "outputs": [],
      "source": [
        "income_model_path = '/content/drive/MyDrive/my_model'\n",
        "output_model_path = \"/content/drive/MyDrive/Diploma/my_model.onnx\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pYj2gnW_PiRk"
      },
      "outputs": [],
      "source": [
        "!pip install onnxruntime\n",
        "!pip install onnxscript\n",
        "!pip install onnx\n",
        "import torch\n",
        "income_model_path = '../my_model'\n",
        "output_model_path = '../my_model.onnx'\n",
        "def export_to_onnx(income_model_path, output_model_path):\n",
        "  torch_model = torch.load(income_model_path)\n",
        "  device = torch.device(\"cuda\")\n",
        "  torch_model = torch_model.to(device)\n",
        "  input_tensor = input_tensor.to(device)\n",
        "  torch.onnx.export(torch_model, input_tensor, output_model_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CmU_x7OFhMl1"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\")\n",
        "torch_model = torch_model.to(device)\n",
        "input_tensor = input_tensor.to(device)\n",
        "torch.onnx.export(torch_model, input_tensor, \"/content/drive/MyDrive/Diploma/my_model.onnx\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "aiNDDQp_G9DE",
        "r08Vkk2AG_fe"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}